# File generated by CompCert 3.5
# Command line: ok_multiargs.lus
	.data
	.align	3
	.globl	_self$
_self$:
	.data
	.align	3
	.globl	_w$
_w$:
	.space	1
	.data
	.align	3
	.globl	_x$
_x$:
	.space	1
	.data
	.align	3
	.globl	_y$
_y$:
	.space	1
	.data
	.align	3
	.globl	_z$
_z$:
	.space	1
	.data
	.align	3
	.globl	_a$
_a$:
	.space	1
	.data
	.align	3
	.globl	_b$
_b$:
	.space	1
	.data
	.align	3
	.globl	_c$
_c$:
	.space	1
	.data
	.align	3
	.globl	_d$
_d$:
	.space	1
	.text
	.align	4
	.globl _fun$swap$step
_fun$swap$step:
	.cfi_startproc
	subl	$12, %esp
	.cfi_adjust_cfa_offset	12
	leal	16(%esp), %eax
	movl	%eax, 0(%esp)
	movl	%ebx, 4(%esp)
	movl	12(%eax), %ecx
	movl	8(%eax), %edx
	movl	4(%eax), %ebx
	cmpl	$0, %ecx
	setne	%al
	movzbl	%al, %eax
	movb	%al, 0(%ebx)
	cmpl	$0, %edx
	setne	%cl
	movzbl	%cl, %ecx
	movb	%cl, 1(%ebx)
	movl	4(%esp), %ebx
	addl	$12, %esp
	ret
	.cfi_endproc
	.text
	.align	4
	.globl _fun$swap$reset
_fun$swap$reset:
	.cfi_startproc
	subl	$12, %esp
	.cfi_adjust_cfa_offset	12
	leal	16(%esp), %eax
	movl	%eax, 0(%esp)
	addl	$12, %esp
	ret
	.cfi_endproc
	.text
	.align	4
	.globl _fun$shuffle$step
_fun$shuffle$step:
	.cfi_startproc
	subl	$60, %esp
	.cfi_adjust_cfa_offset	60
	leal	64(%esp), %eax
	movl	%eax, 16(%esp)
	movl	%ebx, 20(%esp)
	movl	%esi, 24(%esp)
	movl	%edi, 28(%esp)
	movl	%ebp, 32(%esp)
	movl	20(%eax), %edx
	movl	%edx, 44(%esp)
	movl	16(%eax), %esi
	movl	12(%eax), %edi
	movl	8(%eax), %ecx
	movl	4(%eax), %ebx
	movl	0(%eax), %edx
	movl	%edx, 40(%esp)
	leal	50(%esp), %edx
	cmpl	$0, %ecx
	setne	%al
	movzbl	%al, %eax
	movl	%eax, %ebp
	cmpl	$0, %edi
	setne	%cl
	movzbl	%cl, %ecx
	movl	%ecx, 12(%esp)
	movl	%ebp, 8(%esp)
	movl	%edx, 4(%esp)
	movl	40(%esp), %eax
	movl	%eax, 0(%esp)
	call	_fun$swap$step
	movzbl	50(%esp), %ecx
	cmpl	$0, %ecx
	setne	%dl
	movzbl	%dl, %edx
	movb	%dl, 0(%ebx)
	movzbl	51(%esp), %ecx
	cmpl	$0, %ecx
	setne	%al
	movzbl	%al, %eax
	movb	%al, 1(%ebx)
	leal	48(%esp), %edx
	cmpl	$0, %esi
	setne	%al
	movzbl	%al, %eax
	movl	%eax, %esi
	movl	44(%esp), %eax
	cmpl	$0, %eax
	setne	%al
	movzbl	%al, %eax
	movl	%eax, 12(%esp)
	movl	%esi, 8(%esp)
	movl	%edx, 4(%esp)
	movl	40(%esp), %ecx
	movl	%ecx, 0(%esp)
	call	_fun$swap$step
	movzbl	48(%esp), %eax
	cmpl	$0, %eax
	setne	%al
	movzbl	%al, %eax
	movb	%al, 2(%ebx)
	movzbl	49(%esp), %ecx
	cmpl	$0, %ecx
	setne	%dl
	movzbl	%dl, %edx
	movb	%dl, 3(%ebx)
	movl	20(%esp), %ebx
	movl	24(%esp), %esi
	movl	28(%esp), %edi
	movl	32(%esp), %ebp
	addl	$60, %esp
	ret
	.cfi_endproc
	.text
	.align	4
	.globl _fun$shuffle$reset
_fun$shuffle$reset:
	.cfi_startproc
	subl	$28, %esp
	.cfi_adjust_cfa_offset	28
	leal	32(%esp), %eax
	movl	%eax, 4(%esp)
	movl	%ebx, 8(%esp)
	movl	0(%eax), %ebx
	movl	%ebx, 0(%esp)
	call	_fun$swap$reset
	movl	%ebx, 0(%esp)
	call	_fun$swap$reset
	movl	8(%esp), %ebx
	addl	$28, %esp
	ret
	.cfi_endproc
	.text
	.align	4
	.globl _fun$main$step
_fun$main$step:
	.cfi_startproc
	subl	$76, %esp
	.cfi_adjust_cfa_offset	76
	leal	80(%esp), %eax
	movl	%eax, 24(%esp)
	movl	%ebx, 28(%esp)
	movl	%esi, 32(%esp)
	movl	%edi, 36(%esp)
	movl	%ebp, 40(%esp)
	movl	20(%eax), %edi
	movl	16(%eax), %ebp
	movl	12(%eax), %ebx
	movl	8(%eax), %ecx
	movl	4(%eax), %eax
	movl	%eax, 52(%esp)
	movl	24(%esp), %eax
	movl	0(%eax), %eax
	movl	%eax, 48(%esp)
	leal	56(%esp), %edx
	cmpl	$0, %ecx
	setne	%al
	movzbl	%al, %eax
	movl	%eax, %esi
	cmpl	$0, %ebx
	setne	%al
	movzbl	%al, %eax
	movl	%eax, 12(%esp)
	movl	%esi, 8(%esp)
	movl	%edx, 4(%esp)
	movl	48(%esp), %ecx
	movl	%ecx, 0(%esp)
	call	_fun$swap$step
	movzbl	56(%esp), %ebx
	movzbl	57(%esp), %esi
	leal	58(%esp), %edx
	cmpl	$0, %ebp
	setne	%al
	movzbl	%al, %eax
	movl	%eax, %ebp
	cmpl	$0, %edi
	setne	%al
	movzbl	%al, %eax
	movl	%eax, 12(%esp)
	movl	%ebp, 8(%esp)
	movl	%edx, 4(%esp)
	movl	48(%esp), %ecx
	movl	%ecx, 0(%esp)
	call	_fun$swap$step
	movzbl	58(%esp), %ebp
	movzbl	59(%esp), %edi
	leal	64(%esp), %edx
	cmpl	$0, %ebx
	setne	%bl
	movzbl	%bl, %ebx
	cmpl	$0, %esi
	setne	%al
	movzbl	%al, %eax
	movl	%eax, %esi
	cmpl	$0, %ebp
	setne	%al
	movzbl	%al, %eax
	movl	%eax, %ebp
	cmpl	$0, %edi
	setne	%cl
	movzbl	%cl, %ecx
	movl	%ecx, 20(%esp)
	movl	%ebp, 16(%esp)
	movl	%esi, 12(%esp)
	movl	%ebx, 8(%esp)
	movl	%edx, 4(%esp)
	movl	48(%esp), %eax
	movl	%eax, 0(%esp)
	call	_fun$shuffle$step
	movzbl	64(%esp), %ecx
	movzbl	65(%esp), %edx
	movzbl	66(%esp), %edi
	movzbl	67(%esp), %ebp
	leal	60(%esp), %esi
	cmpl	$0, %ecx
	setne	%bl
	movzbl	%bl, %ebx
	cmpl	$0, %edx
	setne	%dl
	movzbl	%dl, %edx
	cmpl	$0, %edi
	setne	%al
	movzbl	%al, %eax
	movl	%eax, %edi
	cmpl	$0, %ebp
	setne	%al
	movzbl	%al, %eax
	movl	%eax, 20(%esp)
	movl	%edi, 16(%esp)
	movl	%edx, 12(%esp)
	movl	%ebx, 8(%esp)
	movl	%esi, 4(%esp)
	movl	48(%esp), %ecx
	movl	%ecx, 0(%esp)
	call	_fun$shuffle$step
	movzbl	60(%esp), %eax
	cmpl	$0, %eax
	setne	%al
	movzbl	%al, %eax
	movl	52(%esp), %edx
	movb	%al, 0(%edx)
	movzbl	61(%esp), %eax
	cmpl	$0, %eax
	setne	%dl
	movzbl	%dl, %edx
	movl	52(%esp), %eax
	movb	%dl, 1(%eax)
	movzbl	62(%esp), %ecx
	cmpl	$0, %ecx
	setne	%al
	movzbl	%al, %eax
	movl	52(%esp), %ecx
	movb	%al, 2(%ecx)
	movzbl	63(%esp), %edx
	cmpl	$0, %edx
	setne	%cl
	movzbl	%cl, %ecx
	movl	52(%esp), %edx
	movb	%cl, 3(%edx)
	movl	28(%esp), %ebx
	movl	32(%esp), %esi
	movl	36(%esp), %edi
	movl	40(%esp), %ebp
	addl	$76, %esp
	ret
	.cfi_endproc
	.text
	.align	4
	.globl _fun$main$reset
_fun$main$reset:
	.cfi_startproc
	subl	$28, %esp
	.cfi_adjust_cfa_offset	28
	leal	32(%esp), %eax
	movl	%eax, 4(%esp)
	movl	%ebx, 8(%esp)
	movl	0(%eax), %ebx
	movl	%ebx, 0(%esp)
	call	_fun$swap$reset
	movl	%ebx, 0(%esp)
	call	_fun$swap$reset
	movl	%ebx, 0(%esp)
	call	_fun$shuffle$reset
	movl	%ebx, 0(%esp)
	call	_fun$shuffle$reset
	movl	8(%esp), %ebx
	addl	$28, %esp
	ret
	.cfi_endproc
	.text
	.align	4
	.globl _main_proved
_main_proved:
	.cfi_startproc
	subl	$60, %esp
	.cfi_adjust_cfa_offset	60
	leal	64(%esp), %eax
	movl	%eax, 24(%esp)
	movl	%ebx, 28(%esp)
	movl	%esi, 32(%esp)
	movl	%edi, 36(%esp)
	movl	%ebp, 40(%esp)
	leal	_self$, %ecx
	movl	%ecx, 0(%esp)
	call	_fun$main$reset
L100:
	movzbl	_a$, %ecx
	movzbl	_b$, %edx
	movzbl	_c$, %ebp
	movzbl	_d$, %esi
	leal	_self$, %ebx
	leal	56(%esp), %eax
	movl	%eax, 48(%esp)
	cmpl	$0, %ecx
	setne	%al
	movzbl	%al, %eax
	movl	%eax, %edi
	cmpl	$0, %edx
	setne	%dl
	movzbl	%dl, %edx
	cmpl	$0, %ebp
	setne	%al
	movzbl	%al, %eax
	movl	%eax, %ebp
	cmpl	$0, %esi
	setne	%al
	movzbl	%al, %eax
	movl	%eax, 20(%esp)
	movl	%ebp, 16(%esp)
	movl	%edx, 12(%esp)
	movl	%edi, 8(%esp)
	movl	48(%esp), %ecx
	movl	%ecx, 4(%esp)
	movl	%ebx, 0(%esp)
	call	_fun$main$step
	movzbl	56(%esp), %eax
	cmpl	$0, %eax
	setne	%al
	movzbl	%al, %eax
	movb	%al, _w$
	movzbl	57(%esp), %eax
	cmpl	$0, %eax
	setne	%al
	movzbl	%al, %eax
	movb	%al, _x$
	movzbl	58(%esp), %eax
	cmpl	$0, %eax
	setne	%al
	movzbl	%al, %eax
	movb	%al, _y$
	movzbl	59(%esp), %eax
	cmpl	$0, %eax
	setne	%al
	movzbl	%al, %eax
	movb	%al, _z$
	jmp	L100
	.cfi_endproc
	.text
	.align	4
	.globl _main
_main:
	.cfi_startproc
	subl	$12, %esp
	.cfi_adjust_cfa_offset	12
	leal	16(%esp), %eax
	movl	%eax, 0(%esp)
	call	_main_proved
	movl	%ebx, %eax
	addl	$12, %esp
	ret
	.cfi_endproc
	.section __IMPORT,__pointers,non_lazy_symbol_pointers
